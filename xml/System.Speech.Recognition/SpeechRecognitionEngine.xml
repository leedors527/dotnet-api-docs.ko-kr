<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="f957a323dec74ac73ed944c20db03f0b11613e4a" /><Meta Name="ms.sourcegitcommit" Value="756d085f27705e86604f1bba5f2086ee23761acf" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="ko-KR" /><Meta Name="ms.lasthandoff" Value="01/30/2019" /><Meta Name="ms.locfileid" Value="55349907" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>프로세스에서 음성 인식 엔진을 관리 하 고 액세스할 수 있는 방법을 제공 합니다.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 설치 된 음성 인식기에 대 한이 클래스의 인스턴스를 만들 수 있습니다. 설치 된 인식기에 대 한 정보를 얻으려면 정적을 사용 하 여 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 메서드.  
  
 이 클래스는를 음성 인식 엔진 in-process로 실행 하 고 다음과 같은 음성 인식의 다양 한 측면에 대 한 제어를 제공 합니다.  
  
-   프로세스에서 음성 인식기를 만들려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> 생성자입니다.  
  
-   음성 인식 문법을 관리 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> 메서드 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> 속성입니다.  
  
-   인식기에 대 한 입력을 구성 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> 메서드.  
  
-   음성 인식 기능을 수행 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 메서드.  
  
-   인식 대기 또는 예기치 않은 입력을 처리 하는 방법을 수정 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다.  
  
-   인식기를 반환 하는 대체 항목의 수를 변경 하려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> 속성입니다. 인식기에서 인식 결과 반환 합니다.는 <xref:System.Speech.Recognition.RecognitionResult> 개체입니다.  
  
-   인식기에 대 한 변경 내용을 동기화에 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 메서드. 인식기 둘 이상의 스레드를 사용 하 여 작업을 수행할 수 있습니다.  
  
-   인식기에 대 한 입력을 에뮬레이션 하기 위해 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 고 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 개체가 인스턴스화된 개체는 프로세스의 유일한 사용 합니다. 반면,는 <xref:System.Speech.Recognition.SpeechRecognizer> 사용 하려는 모든 애플리케이션을 사용 하 여 단일 인식기를 공유 합니다.  
  
> [!NOTE]
>  항상 호출 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> 음성 인식기에 대 한 마지막 참조를 해제 하기 전에 합니다. 가비지 수집기는 인식기 개체를 호출할 때까지 사용 중인 리소스를 해제 되지 것입니다이 고, 그렇지 `Finalize` 메서드.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예에서는 사용 하므로 합니다 `Multiple` 모드는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 메서드에서 수행 인식 콘솔 창을 닫으려면 또는 디버깅을 중지 될 때까지.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 클래스의 새 인스턴스를 초기화합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 생성할 수 있습니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인스턴스 중 하나에서:  
  
-   시스템에 대 한 기본 음성 인식 엔진  
  
-   이름으로 지정 하는 특정 음성 인식 엔진  
  
-   지정 된 로캘에 대 한 기본 음성 인식 엔진  
  
-   지정한 기준을 충족 하는 특정 인식 엔진을 <xref:System.Speech.Recognition.RecognizerInfo> 개체입니다.  
  
 음성 인식기에서 인식을 시작 하기 전에 하나 이상의 음성 인식 문법을 로드 하 고 인식기에 대 한 입력을 구성 해야 합니다.  
  
 문법을 로드를 호출 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
 오디오 입력을 구성 하려면 다음 방법 중 하나를 사용 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>시스템에 대한 기본 음성 인식기를 사용하여 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 클래스의 새 인스턴스를 초기화합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기에 음성 인식 먼저 하나 이상의 인식 문법을 로드 하 고 인식기에 대 한 입력을 구성 해야 합니다.  
  
 문법을 로드를 호출 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
 오디오 입력을 구성 하려면 다음 방법 중 하나를 사용 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">음성 인식기를 지원해야 하는 로캘입니다.</param>
        <summary>지정된 로캘에 대한 기본 음성 인식기를 사용하여 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 클래스의 새 인스턴스를 초기화합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows와 System.Speech API는 모든 유효한 언어-국가 코드를 받습니다. 지정 된 언어를 사용 하 여 음성 인식을 수행 하는 `CultureInfo` 인수, 음성 인식 엔진을 지 원하는 언어-국가 코드를 설치 해야 합니다. Microsoft Windows 7과 함께 제공 되는 음성 인식 엔진은 다음 언어-국가 코드를 사용 하 여 작동 합니다.  
  
-   en-5GB입니다. English (United Kingdom)  
  
-   EN-US입니다. 영어 (미국)  
  
-   de-DE. 독일어 (독일)  
  
-   원본: ES-ES 합니다. 스페인어 (스페인)  
  
-   fr-FR. 프랑스어 (프랑스)  
  
-   JA-JP 합니다. 일본어 (일본)  
  
-   zh-CN. 중국어 (중국)  
  
-   zh-TW. 중국어 (대만)  
  
 "En", "fr"와 같은 두 문자 언어 코드 또는 "es" 허용도 됩니다.  
  
 음성 인식기에서 인식을 시작 하기 전에 하나 이상의 음성 인식 문법을 로드 하 고 인식기에 대 한 입력을 구성 해야 합니다.  
  
 문법을 로드를 호출 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
 오디오 입력을 구성 하려면 다음 방법 중 하나를 사용 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 보여 주고 EN-US 로캘에 대 한 음성 인식기를 초기화 하는 콘솔 애플리케이션 부분을 보여 줍니다.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">설치 된 음성 인식기를 없음 지정 된 로케일을 지원 하거나 <paramref name="culture" /> 고정 문화권입니다.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" />가 <see langword="null" />인 경우</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">특정 음성 인식기에 대한 정보입니다.</param>
        <summary>사용할 인식기를 지정하기 위해 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체의 정보를 사용하는 <see cref="T:System.Speech.Recognition.RecognizerInfo" />의 새 인스턴스를 초기화합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 설치 된 음성 인식기에 대 한이 클래스의 인스턴스를 만들 수 있습니다. 설치 된 인식기에 대 한 정보를 얻으려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 메서드.  
  
 음성 인식기에서 인식을 시작 하기 전에 하나 이상의 음성 인식 문법을 로드 하 고 인식기에 대 한 입력을 구성 해야 합니다.  
  
 문법을 로드를 호출 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
 오디오 입력을 구성 하려면 다음 방법 중 하나를 사용 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 방법을 보여 줍니다 영어 언어를 지 원하는 음성 인식기를 초기화 하는 콘솔 애플리케이션 부분을 보여 줍니다.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">사용할 음성 인식기의 토큰 이름입니다.</param>
        <summary>사용할 인식기의 이름을 지정하는 문자열 매개 변수로 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 클래스의 새 인스턴스를 초기화합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 토큰 이름 값은는 <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> 의 속성을 <xref:System.Speech.Recognition.RecognizerInfo> 에서 반환 된 개체는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> 인식기의 속성. 설치 된 모든 인식기의 컬렉션을 사용 정적 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 메서드.  
  
 음성 인식기에서 인식을 시작 하기 전에 하나 이상의 음성 인식 문법을 로드 하 고 인식기에 대 한 입력을 구성 해야 합니다.  
  
 문법을 로드를 호출 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
 오디오 입력을 구성 하려면 다음 방법 중 하나를 사용 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 다음 예제에서는 콘솔 애플리케이션을 Windows에 대 한 음성 인식기 8.0의 인스턴스를 만들고 기본 음성 인식 방법을 보여 줍니다 부분을 보여 줍니다 (영어-미국)입니다.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">음성 인식기 토큰 이름 가진 설치 또는 <paramref name="recognizerId" />가 빈 문자열 ("").</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" />가 <see langword="null" />인 경우</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />이 수신하는 오디오의 형식을 가져옵니다.</summary>
        <value><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 인스턴스로의 입력에서 오디오 형식, 또는 입력을 구성하지 않았거나 null 입력으로 설정한 경우에는 <see langword="null" /></value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 오디오 입력을 구성 하려면 다음 방법 중 하나를 사용 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 사용 하 여 아래 예제에서는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> 를 받은 오디오 형식 데이터를 표시 합니다.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />이 수신하는 오디오의 수준을 가져옵니다.</summary>
        <value>음성 인식기에 대한 오디오 입력 수준(0에서 100까지)입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 값 0 이러한 유형은 silence, 나타내고 100 최대 입력된 볼륨을 나타냅니다.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />가 오디오 입력 수준을 보고할 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 초당 여러 번이이 이벤트가 발생 합니다. 애플리케이션이 실행 되는 컴퓨터의 이벤트는 발생 빈도 따라 달라 집니다.  
  
 오디오 수준에서 이벤트의 시간을 사용 합니다 <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> 속성은 연결 된 <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>합니다. 현재 오디오 수준의 인식기에 대 한 입력을 사용 하면 인식기의 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> 속성입니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> 대리자를 만드는 경우 이벤트를 처리할 메서드를 결정합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에 대 한 처리기를 추가 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> 이벤트를를 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 개체입니다. 처리기는 콘솔에 새 오디오 수준을 출력합니다.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에 입력을 공급하는 디바이스가 생성하고 있는 오디오 스트림에서 현재 위치를 가져옵니다.</summary>
        <value>입력 디바이스에서 생성 중인 오디오 스트림의 현재 위치입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 속성이 생성 된 해당 오디오 스트림에서 입력된 디바이스의 위치를 참조 합니다. 반면,는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 속성 내의 오디오 입력 인식기의 위치를 참조 합니다. 이러한 위치는 다를 수 있습니다. 예를 들어 인식기에서 받은 입력 하지 있는 it에 아직 경우 인식 결과 다음 값을 생성 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 속성의 값 보다 작으면는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 속성.  
  
   
  
## Examples  
 다음 예제에서는 프로세스에서 음성 인식기는 음성 입력와 일치 하도록 받아쓰기 문법을 사용 합니다. 에 대 한 처리기를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 가 콘솔에 쓰는 이벤트를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> 음성 인식기에서 입력 음성을 감지 하는 경우.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />가 오디오 신호에서 문제를 감지했을 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 어떤 문제가 발생 했습니다.을 사용 합니다 <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> 속성은 연결 된 <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>합니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> 대리자를 만드는 경우 이벤트를 처리할 메서드를 결정합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에서는 정의 대 한 정보를 수집 하는 이벤트 처리기는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> 이벤트입니다.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />이 수신하는 오디오의 상태를 가져옵니다.</summary>
        <value>음성 인식기에 대한 오디오 입력 상태입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> 속성의 멤버를 사용 하 여 오디오 상태를 나타내는 <xref:System.Speech.Recognition.AudioState> 열거형입니다.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />가 수신하는 오디오의 상태가 변경되었을 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 오디오 상태 이벤트의 타임을 사용 합니다 <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> 속성은 연결 된 <xref:System.Speech.Recognition.AudioStateChangedEventArgs>합니다. 인식기에 대 한 입력의 오디오 현재 상태를 가져오려면 인식기를 사용 하 여 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> 속성입니다. 오디오의 상태에 대 한 자세한 내용은 참조는 <xref:System.Speech.Recognition.AudioState> 열거형입니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> 대리자를 만드는 경우 이벤트를 처리할 메서드를 결정합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에서는 대 한 처리기를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> 인식기를 작성 하는 이벤트의 새 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> 콘솔로 때마다의 멤버를 사용 하 여 변경 내용을 <xref:System.Speech.Recognition.AudioState> 열거형입니다.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>인식을 완료하기 전에 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에서 배경 잡음만 포함된 입력만 허용할 시간 간격을 가져오거나 설정합니다.</summary>
        <value>시간 간격의 지속 시간입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 각 음성 인식기에 대기 및 음성 구분 하는 알고리즘이 있습니다. 인식기에는 모든 비-대기 입력 인식기의 초기 규칙이 일치 하지 않는 배경 소음 로드 및 음성 인식 문법 사용 분류 합니다. 블 시간 제한 간격 내에서 배경 소음 및 대기를 수신 하는 인식기를 인식기는 인식 작업을 완료 합니다.  
  
-   인식기를 발생 시킵니다 비동기 인식 작업에 대 한 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트 위치를 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> 속성은 `true`, 및 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> 속성이 `null`.  
  
-   인식기 에뮬레이션와 동기 인식 작업에 대 한 반환 `null`, 유효한 대신 <xref:System.Speech.Recognition.RecognitionResult>합니다.  
  
 블 시간 제한 기간을 0으로 설정 하는 경우 인식기 블 시간 초과 검사를 수행 하지 않습니다. 시간 제한 간격을 임의의 음수가 아닌 값일 수 있습니다. 기본값은 0 초입니다.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 설정 하는 방법을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다.는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 하 고 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 의 속성을 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 음성 인식 시작 하기 전에 합니다. 음성 인식기에 대 한 처리기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트에 이벤트 정보를 보여 주기 위해 콘솔 출력 하는 방법을 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 의 속성을 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인식 작업에 영향을 줍니다.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">이 속성은 0보다 작은 값으로 설정되어 있습니다.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체를 삭제합니다.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체를 삭제합니다.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">관리되는 리소스와 관리되지 않는 리소스를 모두 해제하려면 <see langword="true" />로 설정하고, 관리되지 않는 리소스만 해제하려면 <see langword="false" />로 설정합니다.</param>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체를 삭제하고 세션 중에 사용되는 리소스를 해제합니다.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>동기 음성 인식을 위한 오디오 대신 텍스트를 사용하여 공유 음성 인식기 입력을 모사합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이러한 메서드는 시스템 오디오 입력을 무시 하 고으로 인식기에 텍스트를 제공할 <xref:System.String> 개체 또는 배열로 <xref:System.Speech.Recognition.RecognizedWordUnit> 개체입니다. 이 기능은 테스트 하거나 애플리케이션 또는 문법 디버깅 하는 경우에 유용할 수 있습니다. 예를 들어, 문법에 단어 인지와 단어 인식 되는 의미 체계 반환 됩니다 결정할 에뮬레이션을 사용할 수 있습니다. 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> 에뮬레이션 작업 중 음성 인식 엔진에 대 한 오디오 입력을 사용 하지 않도록 설정 하는 방법입니다.  
  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다. 인식기 새 줄 및 추가 공백을 무시 하 고 리터럴 입력으로 문장 부호를 처리 합니다.  
  
> [!NOTE]
>  합니다 <xref:System.Speech.Recognition.RecognitionResult> 에뮬레이트된 입력에 따라에서 음성 인식기에 의해 생성 된 개체의 값이 `null` 에 대 한 해당 <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 속성입니다.  
  
 비동기 인식의 에뮬레이션 하기 위해 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">인식 작업에 필요한 입력입니다.</param>
        <summary>동기 음성 인식용 오디오 대신 텍스트를 사용하여 음성 인식기에 구 입력을 에뮬레이션합니다.</summary>
        <returns>인식 작업의 결과이거나, 작업이 성공적으로 수행되지 않았거나 인식기를 사용할 수 없는 경우 <see langword="null" />입니다.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다.  
  
 인식기 Vista 및 Windows 7을 사용 하 여 제공 되는 대/소문자 무시 및 문자 너비 입력된 구를 문법 규칙을 적용 합니다. 이 유형의 비교에 대 한 자세한 내용은 참조는 <xref:System.Globalization.CompareOptions> 열거형 값 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 및 <xref:System.Globalization.CompareOptions.IgnoreWidth>합니다. 또한 인식자 새 줄 및 추가 공백을 무시 하 고 문장 부호 리터럴 입력으로 처리 합니다.  
  
   
  
## Examples  
 아래 코드 예제는 에뮬레이트된 입력과 연결 된 인식 결과, 음성 인식기에 의해 발생 하는 연결 된 이벤트를 보여 주는 콘솔 애플리케이션의 일부입니다. 이 예제에서는 다음 출력을 생성 합니다.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">인식기에는 음성 인식 문법이 로드되지 않았습니다.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />가 빈 문자열("")입니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">인식 작업에 필요한 입력을 포함하는 단어 단위의 배열입니다.</param>
        <param name="compareOptions">에뮬레이트된 인식 작업에 사용할 비교 형식을 설명하는 열거형 값의 비트 조합입니다.</param>
        <summary>동기 음성 인식용 오디오 대신 텍스트를 사용하여 음성 인식기에 특정 단어 입력을 에뮬레이션하고, 인식기가 단어와 로드된 음성 인식 문법 사이의 유니코드 비교를 처리할 방법을 지정합니다.</summary>
        <returns>인식 작업의 결과이거나, 작업이 성공적으로 수행되지 않았거나 인식기를 사용할 수 없는 경우 <see langword="null" />입니다.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다.  
  
 인식기를 사용 하 여 `compareOptions` 때 적용할 문법 규칙 입력된 구입니다. 인식기 Vista 및 Windows 7을 사용 하 여 제공 되는 경우 대/소문자를 무시 합니다 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 또는 <xref:System.Globalization.CompareOptions.IgnoreCase> 값이 있음. 인식기 항상 문자 너비를 무시 하 고는 일본어가 나 형식을 무시 하지 않습니다. 인식기는 또한 새 줄 및 추가 공백을 무시 하 고 문장 부호 리터럴 입력으로 처리 합니다. 문자 너비 및 일본어가 나 형식에 대 한 자세한 내용은 참조는 <xref:System.Globalization.CompareOptions> 열거형입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">인식기에는 음성 인식 문법이 로드되지 않았습니다.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" />에는 하나 이상의 <see langword="null" /> 요소가 포함됩니다.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />에는 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 또는 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 플래그가 포함됩니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">인식 작업에 필요한 입력 구입니다.</param>
        <param name="compareOptions">에뮬레이트된 인식 작업에 사용할 비교 형식을 설명하는 열거형 값의 비트 조합입니다.</param>
        <summary>동기 음성 인식용 오디오 대신 텍스트를 사용하여 음성 인식기에 구 입력을 에뮬레이션하고, 인식기가 구와 로드된 음성 인식 문법 사이의 유니코드 비교를 처리할 방법을 지정합니다.</summary>
        <returns>인식 작업의 결과이거나, 작업이 성공적으로 수행되지 않았거나 인식기를 사용할 수 없는 경우 <see langword="null" />입니다.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다.  
  
 인식기를 사용 하 여 `compareOptions` 때 적용할 문법 규칙 입력된 구입니다. 인식기 Vista 및 Windows 7을 사용 하 여 제공 되는 경우 대/소문자를 무시 합니다 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 또는 <xref:System.Globalization.CompareOptions.IgnoreCase> 값이 있음. 인식기 항상 문자 너비를 무시 하 고는 일본어가 나 형식을 무시 하지 않습니다. 인식기는 또한 새 줄 및 추가 공백을 무시 하 고 문장 부호 리터럴 입력으로 처리 합니다. 문자 너비 및 일본어가 나 형식에 대 한 자세한 내용은 참조는 <xref:System.Globalization.CompareOptions> 열거형입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">인식기에는 음성 인식 문법이 로드되지 않았습니다.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />가 빈 문자열("")입니다.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />에는 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 또는 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 플래그가 포함됩니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>비동기 음성 인식을 위한 오디오 대신 텍스트를 사용하여 공유 음성 인식기 입력을 모사합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이러한 메서드는 시스템 오디오 입력을 무시 하 고으로 인식기에 텍스트를 제공할 <xref:System.String> 개체 또는 배열로 <xref:System.Speech.Recognition.RecognizedWordUnit> 개체입니다. 이 기능은 테스트 하거나 애플리케이션 또는 문법 디버깅 하는 경우에 유용할 수 있습니다. 예를 들어, 문법에 단어 인지와 단어 인식 되는 의미 체계 반환 됩니다 결정할 에뮬레이션을 사용할 수 있습니다. 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> 에뮬레이션 작업 중 음성 인식 엔진에 대 한 오디오 입력을 사용 하지 않도록 설정 하는 방법입니다.  
  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다. 인식기에는 비동기 인식 작업이 완료 되 면 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 이벤트입니다. 인식기 새 줄 및 추가 공백을 무시 하 고 리터럴 입력으로 문장 부호를 처리 합니다.  
  
> [!NOTE]
>  합니다 <xref:System.Speech.Recognition.RecognitionResult> 에뮬레이트된 입력에 따라에서 음성 인식기에 의해 생성 된 개체의 값이 `null` 에 대 한 해당 <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 속성입니다.  
  
 동기 인식 기능을 에뮬레이트하는 데 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 메서드.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">인식 작업에 필요한 입력입니다.</param>
        <summary>비동기 음성 인식용 오디오 대신 텍스트를 사용하여 음성 인식기에 구 입력을 에뮬레이션합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다. 인식기에는 비동기 인식 작업이 완료 되 면 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 이벤트입니다.  
  
 인식기 Vista 및 Windows 7을 사용 하 여 제공 되는 대/소문자 무시 및 문자 너비 입력된 구를 문법 규칙을 적용 합니다. 이 유형의 비교에 대 한 자세한 내용은 참조는 <xref:System.Globalization.CompareOptions> 열거형 값 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 및 <xref:System.Globalization.CompareOptions.IgnoreWidth>합니다. 또한 인식자 새 줄 및 추가 공백을 무시 하 고 문장 부호 리터럴 입력으로 처리 합니다.  
  
   
  
## Examples  
 아래 코드 예제를 사용 하면 비동기 에뮬레이트된 입력, 연결 된 인식 결과 및 음성 인식기에 의해 발생 하는 연결 된 이벤트를 보여 주는 콘솔 애플리케이션의 일부인 합니다. 이 예제에서는 다음 출력을 생성 합니다.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">인식기에 음성 인식 문법이 로드되어 있지 않거나 인식기에 아직 완료되지 않은 비동기 인식 작업이 있습니다.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />가 빈 문자열("")입니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">인식 작업에 필요한 입력을 포함하는 단어 단위의 배열입니다.</param>
        <param name="compareOptions">에뮬레이트된 인식 작업에 사용할 비교 형식을 설명하는 열거형 값의 비트 조합입니다.</param>
        <summary>비동기 음성 인식용 오디오 대신 <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> 개체의 배열을 사용하여 음성 인식기에 특정 단어 입력을 에뮬레이션하고, 인식기가 단어와 로드된 음성 인식 문법 사이의 유니코드 비교를 처리할 방법을 지정합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다. 인식기에는 비동기 인식 작업이 완료 되 면 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 이벤트입니다.  
  
 인식기를 사용 하 여 `compareOptions` 때 적용할 문법 규칙 입력된 구입니다. 인식기 Vista 및 Windows 7을 사용 하 여 제공 되는 경우 대/소문자를 무시 합니다 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 또는 <xref:System.Globalization.CompareOptions.IgnoreCase> 값이 있음. 인식자 항상 문자 너비를 무시 하 고 일본어가 나 형식 무시 해서는 안됩니다. 또한 인식자 새 줄 및 추가 공백을 무시 하 고 문장 부호 리터럴 입력으로 처리 합니다. 문자 너비 및 일본어가 나 형식에 대 한 자세한 내용은 참조는 <xref:System.Globalization.CompareOptions> 열거형입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">인식기에 음성 인식 문법이 로드되어 있지 않거나 인식기에 아직 완료되지 않은 비동기 인식 작업이 있습니다.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" />에는 하나 이상의 <see langword="null" /> 요소가 포함됩니다.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />에는 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 또는 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 플래그가 포함됩니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">인식 작업에 필요한 입력 구입니다.</param>
        <param name="compareOptions">에뮬레이트된 인식 작업에 사용할 비교 형식을 설명하는 열거형 값의 비트 조합입니다.</param>
        <summary>비동기 음성 인식용 오디오 대신 텍스트를 사용하여 음성 인식기에 구 입력을 에뮬레이션하고, 인식기가 구와 로드된 음성 인식 문법 사이의 유니코드 비교를 처리할 방법을 지정합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트 인식 작업 에뮬레이트되지 않은 것 처럼 합니다. 인식기에는 비동기 인식 작업이 완료 되 면 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 이벤트입니다.  
  
 인식기를 사용 하 여 `compareOptions` 때 적용할 문법 규칙 입력된 구입니다. 인식기 Vista 및 Windows 7을 사용 하 여 제공 되는 경우 대/소문자를 무시 합니다 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 또는 <xref:System.Globalization.CompareOptions.IgnoreCase> 값이 있음. 인식자 항상 문자 너비를 무시 하 고 일본어가 나 형식 무시 해서는 안됩니다. 또한 인식자 새 줄 및 추가 공백을 무시 하 고 문장 부호 리터럴 입력으로 처리 합니다. 문자 너비 및 일본어가 나 형식에 대 한 자세한 내용은 참조는 <xref:System.Globalization.CompareOptions> 열거형입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">인식기에 음성 인식 문법이 로드되어 있지 않거나 인식기에 아직 완료되지 않은 비동기 인식 작업이 있습니다.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" />가 빈 문자열("")입니다.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" />에는 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 또는 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 플래그가 포함됩니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />가 에뮬레이트된 입력에 대한 비동기 인식 작업을 종결할 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 각 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드는 비동기 인식 작업을 시작 합니다. <xref:System.Speech.Recognition.SpeechRecognitionEngine> 발생을 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 비동기 작업을 완료 하면 이벤트.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 작업 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 이벤트는 마지막 이러한 이벤트 인식기를 지정된 된 작업에 대 한 발생 시킵니다.  
  
 에뮬레이트된 인식에 성공한 경우에 다음 중 하나를 사용 하 여 인식 결과 액세스할 수 있습니다.  
  
-   <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> 속성에는 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> 에 대 한 처리기에서 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 이벤트입니다.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> 속성에는 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> 에 대 한 처리기에서 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트입니다.  
  
 에뮬레이트된 인식 하지 못한 경우에 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트가 발생 하지 않습니다 및 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> null이 됩니다.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>은 <xref:System.ComponentModel.AsyncCompletedEventArgs>로부터 파생됩니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs>은 <xref:System.Speech.Recognition.RecognitionEventArgs>로부터 파생됩니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 대리자를 만드는 경우 이벤트를 처리할 메서드를 결정합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제는 콘솔 애플리케이션을 음성 인식 문법을 로드 및 비동기 에뮬레이트된 입력, 연결 된 인식 결과 및 음성 인식기에 의해 발생 하는 연결 된 이벤트를 보여 줍니다.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>인식 작업을 마치기 전에 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />이 모호하지 않은 입력의 끝에서 허용할 무음 간격을 가져오거나 설정합니다.</summary>
        <value>무음 간격의 지속 시간입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 인식 입력 모호한 경우이 시간 제한 간격을 사용 합니다. 음성 인식 문법을 인식의에 지에 대 한 예를 들어, "새 게임 하세요" 또는 "새 게임"을 "새 하세요 게임" 명확한 입력 이며 "new game" 모호한 입력 합니다.  
  
 이 속성을 인식 작업을 완료 하기 전에 음성 인식 엔진 추가 입력에 대해 대기 하는 기간을 결정 합니다. 시간 제한 간격을 0 초에서 10 초, 포괄 수 있습니다. 기본값은 150 밀리초입니다.  
  
 모호한 입력에 대 한 제한 시간 간격을 설정 하려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">이 속성은 0초보다 작거나 10초보다 큰 값으로 설정되어 있습니다.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>인식 작업을 마치기 전에 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />이 모호한 입력의 끝에서 허용할 무음 간격을 가져오거나 설정합니다.</summary>
        <value>무음 간격의 지속 시간입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식기가 인식 입력 모호한 경우이 시간 제한 간격을 사용 합니다. 음성 인식 문법을 인식의에 지에 대 한 예를 들어, "새 게임 하세요" 또는 "새 게임"을 "새 하세요 게임" 명확한 입력 이며 "new game" 모호한 입력 합니다.  
  
 이 속성을 인식 작업을 완료 하기 전에 음성 인식 엔진 추가 입력에 대해 대기 하는 기간을 결정 합니다. 시간 제한 간격을 0 초에서 10 초, 포괄 수 있습니다. 기본값은 500 밀리초입니다.  
  
 명확한 입력에 대 한 제한 시간 간격을 설정 하려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 속성입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">이 속성은 0초보다 작거나 10초보다 큰 값으로 설정되어 있습니다.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>이 <see cref="T:System.Speech.Recognition.Grammar" /> 인스턴스에 로드된 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체의 컬렉션을 가져옵니다.</summary>
        <value><see cref="T:System.Speech.Recognition.Grammar" /> 개체의 컬렉션입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 다음 예에서는 현재 음성 인식기에 의해 로드 된 각 음성 인식 문법에 대 한 콘솔에 정보를 출력 합니다.  
  
> [!IMPORTANT]
>  이 메서드는 컬렉션의 요소를 열거 하는 동안 컬렉션이 수정 되는 경우 오류를 방지 하려면 문법 컬렉션을 복사 합니다.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>인식을 완료하기 전에 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에서 무음만 포함된 입력만 허용할 시간 간격을 가져오거나 설정합니다.</summary>
        <value>무음 간격의 지속 시간입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 각 음성 인식기에 대기 및 음성 구분 하는 알고리즘이 있습니다. 인식기 입력을 대기 초기 대기 시간 동안 발생 한 경우 인식기는 인식 작업을 완료 합니다.  
  
-   인식기를 발생 시킵니다 에뮬레이션와 비동기 인식 작업에 대 한 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트 위치를 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> 속성은 `true`, 및 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> 속성이 `null`.  
  
-   인식기 에뮬레이션와 동기 인식 작업에 대 한 반환 `null`, 유효한 대신 <xref:System.Speech.Recognition.RecognitionResult>합니다.  
  
 초기 대기 제한 시간 간격을 0으로 설정 하는 경우 인식기에는 초기 대기 제한 시간 검사를 수행 하지 않습니다. 시간 제한 간격을 임의의 음수가 아닌 값일 수 있습니다. 기본값은 0 초입니다.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 예제에서는 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 의 속성을 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 음성 인식 시작 하기 전에 합니다. 음성 인식기에 대 한 처리기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트에 이벤트 정보를 보여 주기 위해 콘솔 출력 하는 방법을 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 의 속성을 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 속성 인식 작업에 영향을 줍니다.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">이 속성은 0보다 작은 값으로 설정되어 있습니다.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>현재 시스템에 설치된 음성 인식기의 모든 정보를 반환합니다.</summary>
        <returns>설치된 인식기를 설명하는 <see cref="T:System.Speech.Recognition.RecognizerInfo" /> 개체의 읽기 전용 컬렉션</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 현재 인식기에 대 한 정보를 얻으려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> 속성입니다.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예제에서는 사용 하 여 반환 된 컬렉션을 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 메서드 영어 언어를 지 원하는 음성 인식기를 합니다.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">로드할 문법 개체입니다.</param>
        <summary><see cref="T:System.Speech.Recognition.Grammar" /> 개체를 동기적으로 로드합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 경우 예외를 throw 합니다 <xref:System.Speech.Recognition.Grammar> 개체 이미 로드 또는 비동기적으로 로드 되 고, 모든 인식기에 로드 하지 못했습니다. 동일한를 로드할 수 없습니다 <xref:System.Speech.Recognition.Grammar> 개체의 여러 인스턴스로 <xref:System.Speech.Recognition.SpeechRecognitionEngine>합니다. 대신 새를 만들 <xref:System.Speech.Recognition.Grammar> 각각에 대 한 개체 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인스턴스.  
  
 인식기를 실행 하는 경우 애플리케이션 사용 해야 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 로드, 언로드을 사용 하도록 설정 또는 문법을 사용 하지 않도록 설정 하기 전에 음성 인식 엔진을 일시 중지 합니다.  
  
 문법을 로드할 때 기본적으로 활성화 됩니다. 로드할된 문법을 사용 하지는 <xref:System.Speech.Recognition.Grammar.Enabled%2A> 속성입니다.  
  
 로드 하는 <xref:System.Speech.Recognition.Grammar> 비동기적으로 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예에서는 만듭니다는 <xref:System.Speech.Recognition.DictationGrammar> 음성 인식기에 로드 합니다.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" />가 유효한 상태가 아닌 경우</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">로드할 음성 인식 문법입니다.</param>
        <summary>음성 인식 문법을 비동기적으로 로드합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기가 로드를 완료 하는 경우는 <xref:System.Speech.Recognition.Grammar> 개체를 발생 시킵니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> 이벤트입니다. 인식기 경우 예외를 throw 합니다 <xref:System.Speech.Recognition.Grammar> 개체 이미 로드 또는 비동기적으로 로드 되 고, 모든 인식기에 로드 하지 못했습니다. 동일한를 로드할 수 없습니다 <xref:System.Speech.Recognition.Grammar> 개체의 여러 인스턴스로 <xref:System.Speech.Recognition.SpeechRecognitionEngine>합니다. 대신 새를 만들 <xref:System.Speech.Recognition.Grammar> 각각에 대 한 개체 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인스턴스.  
  
 인식기를 실행 하는 경우 애플리케이션 사용 해야 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 로드, 언로드을 사용 하도록 설정 또는 문법을 사용 하지 않도록 설정 하기 전에 음성 인식 엔진을 일시 중지 합니다.  
  
 문법을 로드할 때 기본적으로 활성화 됩니다. 로드할된 문법을 사용 하지는 <xref:System.Speech.Recognition.Grammar.Enabled%2A> 속성입니다.  
  
 음성 인식 문법의 동기적으로 로드 하려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 메서드.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" />가 유효한 상태가 아닌 경우</exception>
        <exception cref="T:System.OperationCanceledException">비동기 작업이 취소되었습니다.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />가 <see cref="T:System.Speech.Recognition.Grammar" /> 개체의 비동기 로딩을 완료했을 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드는 비동기 작업을 시작 합니다. <xref:System.Speech.Recognition.SpeechRecognitionEngine> 작업이 완료 되 면이 이벤트가 발생 합니다. 가져오려는 합니다 <xref:System.Speech.Recognition.Grammar> 인식기에 로드 된 개체를 사용 하 여는 <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> 속성은 연결 된 <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>합니다. 현재 가져오려는 <xref:System.Speech.Recognition.Grammar> 인식기를 사용 하는 인식기를 로드 하는 개체 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> 속성입니다.  
  
 인식기를 실행 하는 경우 애플리케이션 사용 해야 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 로드, 언로드을 사용 하도록 설정 또는 문법을 사용 하지 않도록 설정 하기 전에 음성 인식 엔진을 일시 중지 합니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> 대리자를 만들 때, 이벤트를 처리할 메서드를 식별합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에서는 in process 음성 인식기를 만들고 두 가지 유형의 무료 받아쓰기 허용 및 특정 단어를 인식 하는 것에 대 한 문법을 만듭니다. 예에서는 <xref:System.Speech.Recognition.Grammar> 개체에서 각 완료 된 음성 인식 문법에 비동기적으로 로드 합니다 <xref:System.Speech.Recognition.Grammar> 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인스턴스. 인식기에 대 한 처리기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> 하 고 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트의 이름을 콘솔에 기록 합니다 <xref:System.Speech.Recognition.Grammar> 인식 및 텍스트 인식 결과를 각각 수행 하는 개체입니다.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>각 인식 작업을 위해 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에서 반환하는 대체 인식 결과의 최대 개수를 가져오거나 설정합니다.</summary>
        <value>반환할 대체 결과 수입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> 의 속성 합니다 <xref:System.Speech.Recognition.RecognitionResult> 클래스의 컬렉션을 포함 <xref:System.Speech.Recognition.RecognizedPhrase> 해석 될 입력을 나타내는 개체입니다.  
  
 에 대 한 기본값 <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> 은 10입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />가 0보다 작은 값으로 설정된 경우</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">반환할 설정의 이름입니다.</param>
        <summary>인식기에 대한 설정의 값을 반환합니다.</summary>
        <returns>설정의 값입니다.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 설정 문자열, 64 비트 정수 또는 메모리 주소 데이터를 포함할 수 있습니다. 다음 표에서 Microsoft Speech API (SAPI)에 대해 정의 된 설정을-인식기 규정을 준수 합니다. 다음 설정의 설정을 지 원하는 각 인식기에 대 한 동일한 범위를 있어야 합니다. SAPI 규격 인식기는 이러한 설정을 지원 필요가 없습니다 및 기타 설정을 지원할 수 있습니다.  
  
|name|설명|  
|----------|-----------------|  
|`ResourceUsage`|인식기의 CPU 사용량을 지정합니다. 범위는 0에서 100 까지입니다. 기본값은 50입니다.|  
|`ResponseSpeed`|음성 인식기가 인식 작업을 완료 하기 전에 대기 모호 하지 않은 입력 끝의 길이 나타냅니다. 범위는 0에서 10, 000 밀리초 (ms)입니다. 이 설정은 인식기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 속성입니다.  기본 150ms =.|  
|`ComplexResponseSpeed`|음성 인식기가 인식 작업을 완료 하기 전에 대기 모호한 입력의 끝의 길이 나타냅니다. 범위는 0에서 10,000ms 까지입니다. 이 설정은 인식기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다. 기본값은 500 밀리초입니다.|  
|`AdaptationOn`|어쿠스틱 모델 적응 켜져 있는지 여부를 나타냅니다 (값 = `1`) 또는 OFF (값 = `0`). 기본값은 `1` (ON).|  
|`PersistedBackgroundAdaptation`|백그라운드 적응 켜져 있는지 여부를 나타냅니다 (값 = `1`) 또는 OFF (값 = `0`), 레지스트리에서 설정을 유지 합니다. 기본값은 `1` (ON).|  
  
 인식기에 대 한 설정을 업데이트 하려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 메서드.  
  
   
  
## Examples  
 다음 예제는 다양 한 EN-US 로캘을 지 인식기에 대해 정의 된 설정에 대 한 값을 출력 하는 콘솔 애플리케이션의 일부입니다. 이 예제에서는 다음 출력을 생성 합니다.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" />가 빈 문자열("")입니다.</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">인식기에는 해당 이름의 설정이 없습니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>동기 음성 인식 작업을 시작합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이러한 메서드는 단일, 동기 인식 작업을 수행합니다. 인식기는 로드 하 고 사용할 음성 인식 문법에 대해이 작업을 수행합니다.  
  
 이 메서드를 호출 하는 동안 인식기 다음 이벤트를 발생 시킬 수 있습니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  인식기가 음성으로 식별할 수 있는 입력을 감지할 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  입력 active 문법 중 하 나와 일치 상태가 모호를 만들 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 인식기 인식 작업을 종료 하는 경우 발생 합니다.  
  
 인식기 발생 하지 않습니다 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 중 하나를 사용 하는 경우 이벤트는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 메서드.  
  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 메서드는 반환을 <xref:System.Speech.Recognition.RecognitionResult> 개체 또는 `null` 작업이 성공적으로 수행 되지 않거나 인식기를 사용할 수 없습니다.  
  
 동기 인식 작업을 다음과 같은 이유로 실패할 수 있습니다.  
  
-   음성에 대 한 시간 제한 간격이 만료 되기 전에 검색 되지 않은 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 속성을 또는 `initialSilenceTimeout` 의 매개 변수는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 메서드.  
  
-   인식 엔진 음성 검색 했지만 로드 하 고 활성화 중 하나에서 일치 하는 항목을 찾은 <xref:System.Speech.Recognition.Grammar> 개체입니다.  
  
 인식기에서 음성의 인식 관련 하 여 대기 시간을 처리 하는 방법을 수정 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다.  
  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 하나 이상 있어야 <xref:System.Speech.Recognition.Grammar> 인식을 수행 하기 전에 개체를 로드 합니다. 음성 인식 문법을 로드 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
 비동기 인식을 수행 하려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 메서드.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>동기 음성 인식 작업을 실행합니다.</summary>
        <returns>입력에 대한 인식 결과, 또는 작업이 성공적으로 수행되지 않았거나 인식기를 사용할 수 없는 경우 <see langword="null" /></returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이 메서드는 단일 인식 작업을 수행합니다. 인식기는 로드 하 고 사용할 음성 인식 문법에 대해이 작업을 수행합니다.  
  
 이 메서드를 호출 하는 동안 인식기 다음 이벤트를 발생 시킬 수 있습니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  인식기가 음성으로 식별할 수 있는 입력을 감지할 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  입력 active 문법 중 하 나와 일치 상태가 모호를 만들 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 인식기 인식 작업을 종료 하는 경우 발생 합니다.  
  
 인식기 발생 하지 않습니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이 메서드를 사용 하는 경우에 이벤트입니다.  
  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> 메서드가 반환 되는 <xref:System.Speech.Recognition.RecognitionResult> 개체 또는 `null` 작업이 성공한 경우.  
  
 동기 인식 작업을 다음과 같은 이유로 실패할 수 있습니다.  
  
-   음성에 대 한 시간 제한 간격이 만료 되기 전에 검색 되지 않은 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 속성입니다.  
  
-   인식 엔진 음성 검색 했지만 로드 하 고 활성화 중 하나에서 일치 하는 항목을 찾은 <xref:System.Speech.Recognition.Grammar> 개체입니다.  
  
 비동기 인식을 수행 하려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예에서는 만듭니다는 <xref:System.Speech.Recognition.DictationGrammar>는 in process 음성 인식기에 로드 하 고 한 인식 작업을 수행 합니다.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">음성 인식기에서 인식 기능을 완료하기 전의 음소거에 대해서만 입력을 받는 시간 간격입니다.</param>
        <summary>지정한 초기 대기 시간을 사용 하 여 동기 음성 인식 작업을 수행합니다.</summary>
        <returns>입력에 대한 인식 결과, 또는 작업이 성공적으로 수행되지 않았거나 인식기를 사용할 수 없는 경우 <see langword="null" /></returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 음성 인식 엔진에서 지정 된 시간 간격 내에서 음성을 검색 하는 경우 `initialSilenceTimeout` 인수를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> 단일 인식 작업을 수행 하 고 종료 합니다.  합니다 `initialSilenceTimeout` 매개 변수 대체 인식기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 속성입니다.  
  
 이 메서드를 호출 하는 동안 인식기 다음 이벤트를 발생 시킬 수 있습니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  인식기가 음성으로 식별할 수 있는 입력을 감지할 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  입력 active 문법 중 하 나와 일치 상태가 모호를 만들 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 인식기 인식 작업을 종료 하는 경우 발생 합니다.  
  
 인식기 발생 하지 않습니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이 메서드를 사용 하는 경우에 이벤트입니다.  
  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> 메서드가 반환 되는 <xref:System.Speech.Recognition.RecognitionResult> 개체 또는 `null` 작업이 성공한 경우.  
  
 동기 인식 작업을 다음과 같은 이유로 실패할 수 있습니다.  
  
-   음성에 대 한 시간 제한 간격이 만료 되기 전에 검색 되지 않은 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 또는 `initialSilenceTimeout` 매개 변수입니다.  
  
-   인식 엔진 음성 검색 했지만 로드 하 고 활성화 중 하나에서 일치 하는 항목을 찾은 <xref:System.Speech.Recognition.Grammar> 개체입니다.  
  
 비동기 인식을 수행 하려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예에서는 만듭니다는 <xref:System.Speech.Recognition.DictationGrammar>는 in process 음성 인식기에 로드 하 고 한 인식 작업을 수행 합니다.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>비동기 음성 인식 작업을 시작합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이러한 메서드는 단일 또는 여러 수행 비동기 인식 작업 합니다. 인식기는 로드 하 고 사용할 음성 인식 문법에 대 한 각 작업을 수행합니다.  
  
 이 메서드를 호출 하는 동안 인식기 다음 이벤트를 발생 시킬 수 있습니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  인식기가 음성으로 식별할 수 있는 입력을 감지할 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  입력 active 문법 중 하 나와 일치 상태가 모호를 만들 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 인식기 인식 작업을 종료 하는 경우 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 발생 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 작업을 완료 합니다.  
  
 비동기 인식 작업의 결과 검색 하려면 인식기의 이벤트 처리기를 연결할 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트입니다. 인식기를 동기 또는 비동기 인식 작업을 성공적으로 완료 될 때마다이 이벤트를 발생 시킵니다. 인식 하지 못한 경우에 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> 속성을 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 처리기에 액세스할 수 있는 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트 됩니다 `null`.  
  
 비동기 인식 작업을 다음과 같은 이유로 실패할 수 있습니다.  
  
-   음성에 대 한 시간 제한 간격이 만료 되기 전에 검색 되지 않은 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 속성입니다.  
  
-   인식 엔진 음성 검색 했지만 로드 하 고 활성화 중 하나에서 일치 하는 항목을 찾은 <xref:System.Speech.Recognition.Grammar> 개체입니다.  
  
-   합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 하나 이상 있어야 <xref:System.Speech.Recognition.Grammar> 인식을 수행 하기 전에 개체를 로드 합니다. 음성 인식 문법을 로드 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 메서드.  
  
-   인식기에서 음성의 인식 관련 하 여 대기 시간을 처리 하는 방법을 수정 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다.  
  
-   동기 인식을 수행 하려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 메서드.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>단일 비동기 음성 인식 작업을 실행합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이 메서드는 단일, 비동기 인식 작업을 수행합니다. 인식기는 로드 하 고 사용할 음성 인식 문법에 대해 작업을 수행합니다.  
  
 이 메서드를 호출 하는 동안 인식기 다음 이벤트를 발생 시킬 수 있습니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  인식기가 음성으로 식별할 수 있는 입력을 감지할 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  입력 active 문법 중 하 나와 일치 상태가 모호를 만들 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 인식기 인식 작업을 종료 하는 경우 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 발생 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 작업을 완료 합니다.  
  
 비동기 인식 작업의 결과 검색 하려면 인식기의 이벤트 처리기를 연결할 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트입니다. 인식기를 동기 또는 비동기 인식 작업을 성공적으로 완료 될 때마다이 이벤트를 발생 시킵니다. 인식 하지 못한 경우에 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> 속성을 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 처리기에 액세스할 수 있는 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트 됩니다 `null`.  
  
 동기 인식을 수행 하려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 기본 비동기 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예에서는 만듭니다는 <xref:System.Speech.Recognition.DictationGrammar>는 in process 음성 인식기에 로드 하 고 하나의 비동기 인식 작업을 수행 합니다. 이벤트 처리기 인식기 작업 중 발생 하는 이벤트를 보여 주기 위해 포함 됩니다.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">하나 이상의 인식 작업을 수행 하는지 여부를 나타냅니다.</param>
        <summary>비동기 음성 인식 작업을 실행합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 경우 `mode` 은 <xref:System.Speech.Recognition.RecognizeMode.Multiple>, 인식기 계속 될 때까지 비동기 인식 작업을 수행 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> 메서드가 호출 됩니다.  
  
 이 메서드를 호출 하는 동안 인식기 다음 이벤트를 발생 시킬 수 있습니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  인식기가 음성으로 식별할 수 있는 입력을 감지할 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  입력 active 문법 중 하 나와 일치 상태가 모호를 만들 때 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 인식기 인식 작업을 종료 하는 경우 발생 합니다.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 발생 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 작업을 완료 합니다.  
  
 비동기 인식 작업의 결과 검색 하려면 인식기의 이벤트 처리기를 연결할 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트입니다. 인식기를 동기 또는 비동기 인식 작업을 성공적으로 완료 될 때마다이 이벤트를 발생 시킵니다. 인식 하지 못한 경우에 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> 속성을 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 처리기에 액세스할 수 있는 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트 됩니다 `null`.  
  
 비동기 인식 작업을 다음과 같은 이유로 실패할 수 있습니다.  
  
-   음성에 대 한 시간 제한 간격이 만료 되기 전에 검색 되지 않은 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 속성입니다.  
  
-   인식 엔진 음성 검색 했지만 로드 하 고 활성화 중 하나에서 일치 하는 항목을 찾은 <xref:System.Speech.Recognition.Grammar> 개체입니다.  
  
 동기 인식을 수행 하려면 중 하나를 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 기본 비동기 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예에서는 만듭니다는 <xref:System.Speech.Recognition.DictationGrammar>는 in process 음성 인식기에 로드 하 고 여러 비동기 인식 작업을 수행 합니다. 비동기 작업은 30 초 후 취소 됩니다. 이벤트 처리기 인식기 작업 중 발생 하는 이벤트를 보여 주기 위해 포함 됩니다.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>현재 인식 작업이 완료되기를 기다리지 않고 비동기 인식을 종료합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이 메서드는 즉시 비동기 인식을 종료합니다. 현재 비동기 인식 작업 입력을 받고, 입력 잘리고 기존 입력을 사용 하 여 작업을 완료 합니다. 인식기를 발생 시킵니다를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 비동기 작업이 취소 되 고 설정 하는 경우 이벤트는 <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> 의 속성을 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 를 `true`. 이 메서드는 시작한 비동기 작업을 취소 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 고 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드.  
  
 입력을 자르지 않고 비동기 인식을 중지 하려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 사용 방법을 보여 주는 콘솔 애플리케이션의 일부를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> 메서드. 예제 및 음성 인식 문법을 로드, 비동기 인식 작업을 계속 시작 만들고 작업을 취소 하기 전에 2 초 일시 중지 합니다. 인식기 파일에서 입력을 받을 c:\temp\audioinput\sample.wav 합니다. 이벤트 처리기 인식기 작업 중 발생 하는 이벤트를 보여 주기 위해 포함 됩니다.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>현재 인식 작업을 종료한 후 비동기 인식을 중지합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 이 메서드는 입력을 자르지 않고 비동기 인식을 종료 합니다. 현재 비동기 인식 작업 입력에 수신 되는 경우 인식기 입력을 현재 인식 작업이 완료 될 때까지 적용을 계속 합니다. 인식기를 발생 시킵니다를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 비동기 작업이 중지 되 고 설정 하는 경우 이벤트를 <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> 의 속성을 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 에 `true`. 이 메서드는 시작한 비동기 작업을 중지 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 고 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드.  
  
 기존 입력만을 사용 하 여 비동기 인식의 즉시 취소 하려면를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 사용 방법을 보여 주는 콘솔 애플리케이션의 일부를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> 메서드. 예제 및 음성 인식 문법을 로드, 비동기 인식 작업을 계속 시작 만들고 작업을 중지 하기 전에 2 초 일시 중지 합니다. 인식기 파일에서 입력을 받을 c:\temp\audioinput\sample.wav 합니다. 이벤트 처리기 인식기 작업 중 발생 하는 이벤트를 보여 주기 위해 포함 됩니다.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />가 에뮬레이트된 입력에 대한 비동기 인식 작업을 종결할 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 개체의 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 메서드 비동기 인식 작업을 시작 합니다. 비동기 작업을 완료 하는 인식기를이 이벤트를 발생 시킵니다.  
  
 에 대 한 처리기를 사용 하는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 이벤트에 액세스할 수 있습니다를 <xref:System.Speech.Recognition.RecognitionResult> 에 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 개체입니다. 인식 하지 못한 경우, <xref:System.Speech.Recognition.RecognitionResult> 됩니다 `null`합니다. 시간 초과 또는 오디오 입력 작업을 중단 발생 실패를 인식 하는지 여부를 확인 하려면에 대 한 속성에 액세스할 수 있습니다 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>하십시오 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, 또는 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>합니다.  
  
 자세한 내용은 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 클래스를 참조하십시오.  
  
 최상의 거부 된 인식 후보에 대 한 세부 정보를 가져오려면 연결에 대 한 처리기를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 이벤트입니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 대리자를 만들 때, 이벤트를 처리할 메서드를 식별합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에서는 "재즈 범주의 artist 목록을 표시" 또는 "앨범 절대적인 표시"와 같은 구를 인식 합니다. 이 예제에서는 사용에 대 한 처리기를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 콘솔에서 결과를 인식 하는 방법에 대 한 정보를 표시 하는 이벤트입니다.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>처리하고 있는 오디오 입력 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />의 현재 위치를 가져옵니다.</summary>
        <value>처리 중인 오디오 입력 인식기의 위치입니다.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 오디오 위치는 각 음성 인식기에 특정 합니다. 입력 스트림의 0 값을 사용 하는 경우에 설정 됩니다.  
  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 속성 참조는 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 오디오 입력 내에서 개체의 위치입니다. 반면,는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 속성이 생성 된 해당 오디오 스트림에서 입력된 디바이스의 위치를 참조 합니다. 이러한 위치는 다를 수 있습니다. 예를 들어 인식기에서 받은 입력 하지 있는 it에 아직 경우 인식 결과 다음 값을 생성 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 속성의 값 보다 작으면는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 속성.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />의 현재 인스턴스에 대한 정보를 가져옵니다.</summary>
        <value>현재 음성 인식기에 대한 정보.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 사용 하 여 현재 시스템에 대 한 모든 설치 된 음성 인식기에 대 한 정보를 가져오려고 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 메서드.  
  
   
  
## Examples  
 다음 예에서는 현재 프로세스에서 음성 인식 엔진에 대 한 데이터의 부분 목록을 가져옵니다. 자세한 내용은 <xref:System.Speech.Recognition.RecognizerInfo>을 참조하세요.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>실행이 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 수정을 적용하기 위해 일시 중지될 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 애플리케이션을 사용 해야 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 를 실행 중인 인스턴스 일시 중지 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 해당 설정을 수정 하기 전에 또는 <xref:System.Speech.Recognition.Grammar> 개체입니다. <xref:System.Speech.Recognition.SpeechRecognitionEngine> 수정을 적용 하기 위해 준비 되 면이 이벤트가 발생 합니다.  
  
 하지만 예를 들어를 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 는 일시 중지 된 로드, 언로드를 활성화 및 비활성화할 수 있습니다 <xref:System.Speech.Recognition.Grammar> 개체 및에 대 한 값을 수정 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 속성. 자세한 내용은 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 메서드를 참조하세요.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 대리자를 만들 때, 이벤트를 처리할 메서드를 식별합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에서는 콘솔 애플리케이션을 로드 하 고 언로드합니다 <xref:System.Speech.Recognition.Grammar> 개체입니다. 애플리케이션을 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 음성 인식 엔진 업데이트를 받을 수 있도록 일시 중지를 요청 하는 방법입니다. 애플리케이션을 로드 하거나 언로드합니다를 <xref:System.Speech.Recognition.Grammar> 개체입니다.  
  
 각 업데이트에 대 한 처리기에서 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이름 및 현재 로드 된 상태의 이벤트 기록 <xref:System.Speech.Recognition.Grammar> 콘솔에는 개체입니다. 문법, 로드 및 언로드될 때 팜 동물의 이름은 팜에 동물 이름과 과일을 차례로 과일의 이름만 먼저 애플리케이션에 인식 합니다.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>인식기가 상태 업데이트를 멈출 것을 요청합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기에 대 한 변경 내용을 동기화 하려면이 메서드를 사용 합니다. 로드 하거나 언로드하면 스피치 인식 그래 머 인식기 처리 하는 동안 입력을 하는 경우 예를 들어이 메서드를 사용 하며 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 인식기의 상태를 사용 하 여 애플리케이션 동작을 동기화 하는 이벤트입니다.  
  
 이 메서드를 호출 하는 경우 인식기 일시 중지 또는 비동기 작업을 완료 하 고 생성 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트입니다. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트 처리기 상태 인식 작업 사이 인식기를 수정할 수 있습니다. 처리할 때 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트 처리기가 반환 될 때까지 이벤트를를 인식기를 놓을 합니다.  
  
> [!NOTE]
>  인식기에 대 한 입력 인식기 발생 하기 전에 변경 되는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 요청 이벤트는 삭제 됩니다.  
  
 이 메서드가 호출 될 때를 나타냅니다.  
  
-   인식기를 즉시 생성 인식기 입력을 처리 하지 않은, 경우를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트입니다.  
  
-   인식기 대기 또는 배경 소음으로 구성 된 입력을 처리 하 고, 있으면 인식기 인식 작업을 일시 중지 하 고 생성 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트입니다.  
  
-   인식기 인식 작업을 완료 한 다음 생성 인식기 입력 하지 않았기 대기 또는 배경 소음을 처리 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트입니다.  
  
 인식기 처리 하는 동안는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트:  
  
-   인식기에는 입력 및의 값을 처리 하지 않습니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 속성 동일 하 게 유지 합니다.  
  
-   인식기 입력을 수집 하 고 값을 계속 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 속성을 변경할 수 있습니다.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>인식기가 상태 업데이트를 멈출 것을 요청합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 생성 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트를 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> 의 속성을 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> 는 `null`.  
  
 사용자 토큰을 제공 하려면 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 메서드. 오디오 위치 오프셋을 지정 하려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 콘솔 애플리케이션을 로드 하 고 언로드합니다 <xref:System.Speech.Recognition.Grammar> 개체입니다. 애플리케이션을 사용 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 음성 인식 엔진 업데이트를 받을 수 있도록 일시 중지를 요청 하는 방법입니다. 애플리케이션을 로드 하거나 언로드합니다를 <xref:System.Speech.Recognition.Grammar> 개체입니다.  
  
 각 업데이트에 대 한 처리기에서 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이름 및 현재 로드 된 상태의 이벤트 기록 <xref:System.Speech.Recognition.Grammar> 콘솔에는 개체입니다. 문법, 로드 및 언로드될 때 팜 동물의 이름은 팜에 동물 이름과 과일을 차례로 과일의 이름만 먼저 애플리케이션에 인식 합니다.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">작업에 대한 정보가 포함된 사용자 정의 정보입니다.</param>
        <summary>인식기가 상태를 업데이트하기 위해 일시 중지되고 관련된 이벤트에 대한 사용자 토큰을 제공하도록 요청합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 생성 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트를 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> 의 속성을 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> 의 값을 포함는 `userToken` 매개 변수.  
  
 오디오 위치 오프셋을 지정 하려면 사용 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 메서드.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">작업에 대한 정보가 포함된 사용자 정의 정보입니다.</param>
        <param name="audioPositionAheadToRaiseUpdate">요청을 지연하는 현재 <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />의 오프셋입니다.</param>
        <summary>인식기가 상태를 업데이트하기 위해 일시 중지되고 관련된 이벤트에 대한 오프셋과 사용자 토큰을 제공하도록 요청합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기가 인식기까지 인식기 업데이트 요청을 시작 하지 않습니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 현재 equals <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`합니다.  
  
 인식기 생성 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 이벤트를 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> 의 속성을 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> 의 값을 포함는 `userToken` 매개 변수.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">오디오 입력 스트림입니다.</param>
        <param name="audioFormat">오디오 입력의 형식입니다.</param>
        <summary>오디오 스트림에서 입력을 받도록 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체를 구성합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 인식 작업을 하는 동안 입력 스트림의 끝에 도달 하는 경우 사용 가능한 입력을 사용 하 여 인식 작업을 완료 합니다. 인식기에 대 한 입력을 업데이트 하지 않는 한 모든 후속 인식 작업에서 예외를 생성할 수 있습니다.  
  
   
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. Example.wav 구와 포함 된 오디오 파일에서 입력을 사용 하는 예제 "하나를 테스트할 테스트 두 세" 및 "미스터 cooper", 일시 중지를 구분 합니다. 이 예제에서는 다음 출력을 생성 합니다.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>기본 오디오 디바이스에서 입력을 받도록 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체를 구성합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 다음 예제에서는 기본 음성 인식 기능을 보여 주는 콘솔 애플리케이션 부분을 보여 줍니다. 이 예제에서는 기본 오디오 디바이스에서 출력을 사용 하 여, 여러 수행 비동기 인식 작업 및 사용자 utters 구의 종료 "종료"입니다.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>음성 인식기에서 입력을 사용할 수 없습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 구성 된 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 사용 하는 경우 입력에 대 한 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드를 일시적으로 오프 라인 인식 엔진을 만들 때 또는 합니다.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">입력으로 사용할 파일의 경로입니다.</param>
        <summary>Waveform 오디오 형식(.wav) 파일에서 입력을 받도록 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체를 구성합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 인식 작업을 하는 동안 입력된 파일의 끝에 도달 하는 경우 사용 가능한 입력을 사용 하 여 인식 작업을 완료 합니다. 인식기에 대 한 입력을 업데이트 하지 않는 한 모든 후속 인식 작업에서 예외를 생성할 수 있습니다.  
  
   
  
## Examples  
 다음 예제에서는.wav 파일에서 오디오 인식을 수행 하 고 인식된 된 텍스트를 콘솔에 씁니다.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">오디오 데이터가 들어 있는 스트림입니다.</param>
        <summary>Waveform 오디오 형식(.wav) 데이터가 포함된 스트림에서 입력을 받도록 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 개체를 구성합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 인식 작업을 하는 동안 입력 스트림의 끝에 도달 하는 경우 사용 가능한 입력을 사용 하 여 인식 작업을 완료 합니다. 인식기에 대 한 입력을 업데이트 하지 않는 한 모든 후속 인식 작업에서 예외를 생성할 수 있습니다.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />가 음성으로 식별할 수 있는 입력을 감지할 때 발생했습니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 각 음성 인식기에 대기 및 음성 구분 하는 알고리즘이 있습니다. 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 음성 인식 작업을 수행를 발생 시킵니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 알고리즘 음성 입력을 식별 하는 경우에 이벤트입니다. 합니다 <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> 속성은 연결 된 <xref:System.Speech.Recognition.SpeechDetectedEventArgs> 개체를 인식기에서 음성을 검색 하는 경우 입력 스트림의 위치를 나타냅니다. <xref:System.Speech.Recognition.SpeechRecognitionEngine> 발생 시킵니다 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 이벤트 중 하나를 발생 시키기 전에 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 이벤트입니다.  
  
 자세한 내용은 참조는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 대리자를 만들 때, 이벤트를 처리할 메서드를 식별합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제는 항공편에 대 한 원본 및 대상 도시를 선택 하는 것에 대 한 콘솔 애플리케이션의 일부입니다. 애플리케이션 "시카고에 마이애미에서 fly 하려고 합니다."와 같은 구를 인식합니다  예제에서는 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 보고서에는 이벤트는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 각 시간 음성이 감지 되는 합니다.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에서 단어나 문법적으로 여러 구의 구성 요소가 될 수 있는 단어를 인식했을 때 발생</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 다양 한 생성 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 하므로 이벤트는 입력된 구를 식별 하려고 합니다. 부분적으로 인식 된 구의 텍스트에 액세스할 수 있습니다는 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> 의 속성을 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> 개체에 대 한 처리기에서는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 이벤트. 일반적으로 이러한 이벤트를 처리는 디버깅에 대해서만 유용 합니다.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs>은 <xref:System.Speech.Recognition.RecognitionEventArgs>로부터 파생됩니다.  
  
 자세한 내용은 참조는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 메서드.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 대리자를 만들 때, 이벤트를 처리할 메서드를 식별합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에서는 "재즈 범주의 artist 목록 표시"와 같은 구를 인식합니다. 이 예제에서는 사용을 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 인식 되는 대로 불완전 한 구 조각을 콘솔에서 표시할 이벤트입니다.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에서 로드하고 활성화한 어떤 <see cref="T:System.Speech.Recognition.Grammar" /> 개체와도 일치하지 않는 입력을 받을 때 발생</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 입력 일치 하지 않음을 충분 한 신뢰를 사용 하 여 로드 하 고 활성화 중 하나를 결정 하는 경우이 이벤트를 발생 시킵니다 <xref:System.Speech.Recognition.Grammar> 개체입니다. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> 의 속성을 <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> 거부 포함 <xref:System.Speech.Recognition.RecognitionResult> 개체입니다. 에 대 한 처리기를 사용할 수는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 인식 검색할 이벤트 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> 는 거부 된 및 <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> 점수입니다.  
  
 애플리케이션을 사용 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인스턴스는 음성 입력을 수락 또는 거부할 중 하나를 사용 하 여 신뢰 수준을 수정할 수 있습니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 메서드. 음성 인식 비 음성 사용 하 여 입력에 반응 하는 방법을 수정할 수 있습니다 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 대리자를 만들 때, 이벤트를 처리할 메서드를 식별합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제에서는 "재즈 범주의 artist 목록을 표시" 또는 "앨범 절대적인 표시"와 같은 구를 인식 합니다. 이 예제에서는 사용에 대 한 처리기를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 음성 입력 충분 한 문법의 내용에 일치 시킬 수 없는 경우 콘솔에서 알림을 표시 하는 이벤트 <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> 성공적인 인식을 생성. 인식 결과 처리기에도 표시 됩니다 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> 는 낮은 신뢰도 점수 때문에 거부 되었습니다.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에서 로드하고 활성화한 어떤 <see cref="T:System.Speech.Recognition.Grammar" /> 개체와 일치하는 입력을 받을 때 발생</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 중 하나를 사용 하 여 인식 작업을 시작할 수 있습니다 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 또는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 메서드. 인식기가 발생 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트는 입력 중 하 나와 일치 해당 로드를 결정 하는 경우 <xref:System.Speech.Recognition.Grammar> 충분 한 정도의 자신감을 인식 구성를 사용 하 여 개체. 합니다 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> 의 속성을 <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> 포함 허용 되 <xref:System.Speech.Recognition.RecognitionResult> 개체입니다. 처리기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트는 인식된 된 구와 뿐만 아니라 인식의 목록을 가져올 수 있습니다 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> 낮은 신뢰도 점수와 함께 합니다.  
  
 애플리케이션을 사용 하는 경우는 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인스턴스는 음성 입력을 수락 또는 거부할 중 하나를 사용 하 여 신뢰 수준을 수정할 수 있습니다는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 메서드.  음성 인식 비 음성 사용 하 여 입력에 반응 하는 방법을 수정할 수 있습니다 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다.  
  
 인식기 문법에 일치 하는 입력을 받을 때 합니다 <xref:System.Speech.Recognition.Grammar> 개체를 발생 시킬 수 해당 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 이벤트입니다. 합니다 <xref:System.Speech.Recognition.Grammar> 개체의 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 이벤트는 음성 인식기의 이전 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트입니다. 특정 문법과 관련 된 모든 작업에 대 한 처리기가 항상 수행 해야 합니다 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 이벤트입니다.  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 대리자를 만들 때, 이벤트를 처리할 메서드를 식별합니다. 이벤트를 이벤트 처리기와 연결하려면 대리자의 인스턴스를 해당 이벤트에 추가합니다. 대리자를 제거하지 않는 경우 이벤트가 발생할 때마다 이벤트 처리기가 호출됩니다. 이벤트 처리기 대리자에 대 한 자세한 내용은 참조 하세요. [이벤트 및 대리자](https://go.microsoft.com/fwlink/?LinkId=162418)합니다.  
  
   
  
## Examples  
 다음 예제는 음성 인식 문법 구문을 만드는 콘솔 애플리케이션의 일부를 <xref:System.Speech.Recognition.Grammar> 개체를 로드 하는 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인식을 수행할 수입니다. 예제에 대 한 음성 입력에는 <xref:System.Speech.Recognition.SpeechRecognitionEngine>, 연결 된 인식 결과 및 음성 인식기에 의해 발생 하는 연결 된 이벤트입니다.  
  
 "시카고에서 마이애미 갈"을 트리거할 같은 입력 음성는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트입니다. "Fly me Houston에서 시카고로" 라는 문구를 말하기 트리거되지는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 이벤트입니다.  
  
 이 예제에서는 사용에 대 한 처리기를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 성공적으로 표시할 이벤트 구 및 콘솔에 포함 된 의미 체계를 인식 합니다.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>모든 <see cref="T:System.Speech.Recognition.Grammar" /> 개체를 인식기에서 언로드합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 현재 로드 하는 경우는 <xref:System.Speech.Recognition.Grammar> 이 메서드 될 때까지 대기 하는 비동기적으로 <xref:System.Speech.Recognition.Grammar> 모든 언로드합니다 전에 로드 되는 <xref:System.Speech.Recognition.Grammar> 에서 개체를 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 인스턴스.  
  
 특정 문법을 언로드 작업을 사용 하 여를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 음성 인식 문법의 동기 로드와 언로드를 보여주는 콘솔 애플리케이션 부분을 보여 줍니다.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">언로드할 문법 개체입니다.</param>
        <summary>지정된 <see cref="T:System.Speech.Recognition.Grammar" /> 개체를 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 인스턴스에서 언로드합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기를 실행 하는 경우 애플리케이션 사용 해야 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 일시 중지 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 로드, 언로드을 사용 하도록 설정 하거나 사용 하지 않도록 설정 하기 전에 인스턴스를 <xref:System.Speech.Recognition.Grammar> 개체입니다. 모든 언로드할 <xref:System.Speech.Recognition.Grammar> 개체를 사용 하 여를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> 메서드.  
  
   
  
## Examples  
 다음 예제에서는 음성 인식 문법의 동기 로드와 언로드를 보여주는 콘솔 애플리케이션 부분을 보여 줍니다.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.InvalidOperationException">문법이 이 인식기에 로드되지 않거나 이 인식기에서 문법을 현재 비동기식으로 로드 중입니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>인식기에 대한 설정 값을 업데이트합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 인식기 설정 문자열, 64 비트 정수 또는 메모리 주소 데이터를 포함할 수 있습니다. 다음 표에서 Microsoft Speech API (SAPI)에 대해 정의 된 설정을-인식기 규정을 준수 합니다. 다음 설정의 설정을 지 원하는 각 인식기에 대 한 동일한 범위를 있어야 합니다. SAPI 규격 인식기는 이러한 설정을 지원 필요가 없습니다 및 기타 설정을 지원할 수 있습니다.  
  
|name|설명|  
|----------|-----------------|  
|`ResourceUsage`|인식기의 CPU 사용량을 지정합니다. 범위는 0에서 100 까지입니다. 기본값은 50입니다.|  
|`ResponseSpeed`|음성 인식기가 인식 작업을 완료 하기 전에 대기 모호 하지 않은 입력 끝의 길이 나타냅니다. 범위는 0에서 10, 000 밀리초 (ms)입니다. 이 설정은 인식기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 속성입니다. 기본 150ms =.|  
|`ComplexResponseSpeed`|음성 인식기가 인식 작업을 완료 하기 전에 모호한 입력의 끝에 대기 (밀리초)의 길이 나타냅니다. 범위는 0에서 10,000ms 까지입니다. 이 설정은 인식기 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다. 기본값은 500 밀리초입니다.|  
|`AdaptationOn`|어쿠스틱 모델 적응 켜져 있는지 여부를 나타냅니다 (값 = `1`) 또는 OFF (값 = `0`). 기본값은 `1` (ON).|  
|`PersistedBackgroundAdaptation`|백그라운드 적응 켜져 있는지 여부를 나타냅니다 (값 = `1`) 또는 OFF (값 = `0`), 레지스트리에서 설정을 유지 합니다. 기본값은 `1` (ON).|  
  
 사용할 인식기의 설정 중 하나를 반환 하는 <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> 메서드.  
  
 경우는 예외 `PersistedBackgroundAdaptation`, 속성 값을 사용 하 여 설정 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 메서드는 현재 인스턴스의 대해서만 계속 적용 됩니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine>, 기본 설정으로 되돌리기는 후 합니다.  
  
 음성 인식 비 음성 사용 하 여 입력에 반응 하는 방법을 수정할 수 있습니다 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>를 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, 및 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 속성입니다.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">업데이트할 설정의 이름입니다.</param>
        <param name="updatedValue">설정의 새 값입니다.</param>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />에 지정된 설정을 지정된 정수 값으로 업데이트합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 경우는 예외 `PersistedBackgroundAdaptation`, 속성 값을 사용 하 여 설정 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 메서드는 현재 인스턴스의 대해서만 계속 적용 됩니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine>, 기본 설정으로 되돌리기는 후 합니다. 참조 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 지원 되는 설정에 대 한 설명입니다.  
  
   
  
## Examples  
 다음 예제는 다양 한 EN-US 로캘을 지 인식기에 대해 정의 된 설정에 대 한 값을 출력 하는 콘솔 애플리케이션의 일부입니다. 예제는 신뢰도 수준 설정을 업데이트 하 고 업데이트 된 값을 확인 하는 인식기를 쿼리 합니다. 이 예제에서는 다음 출력을 생성 합니다.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" />가 빈 문자열("")입니다.</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">인식기에는 해당 이름의 설정이 없습니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">업데이트할 설정의 이름입니다.</param>
        <param name="updatedValue">설정의 새 값입니다.</param>
        <summary>지정된 음성 인식 엔진 설정을 지정된 문자열 값으로 업데이트합니다.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 경우는 예외 `PersistedBackgroundAdaptation`, 속성 값을 사용 하 여 설정 합니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 메서드는 현재 인스턴스의 대해서만 계속 적용 됩니다 <xref:System.Speech.Recognition.SpeechRecognitionEngine>, 기본 설정으로 되돌리기는 후 합니다. 참조 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 지원 되는 설정에 대 한 설명입니다.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" />가 <see langword="null" />인 경우</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" />가 빈 문자열("")입니다.</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">인식기에는 해당 이름의 설정이 없습니다.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>